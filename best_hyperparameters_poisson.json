{
    "activation_type": "relu",
    "num_layers": 2,
    "units_layer_0": 25,
    "dropout_rate": 0.5,
    "units_layer_1": 20,
    "learning_rate": 0.01,
    "units_layer_2": 15,
    "batch_size": 15000,
    "units_layer_3": 25
}